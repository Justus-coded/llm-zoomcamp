{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f790cd5-b08f-4ed5-ab4c-857d154d1868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import minsearch\n",
    "import json\n",
    "import os\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0491adeb-64e6-47c7-a22e-6b2c690ed5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7cb983f-a602-478f-8049-f4e1c6d6f851",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78599cbf-3e63-4de2-9989-e510e1260484",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('documents.json','rt') as f_in:\n",
    "    docs_raw = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4b10823-679f-4ef5-a960-5f6a7b4e0686",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents =[]\n",
    "\n",
    "for course_dict in docs_raw:\n",
    "    for doc in course_dict['documents']:\n",
    "        doc['course'] = course_dict['course']\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d90b572e-fc41-4285-8beb-1a9761d65675",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7fe2877-8257-4e50-bcdd-efcf0f555cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = minsearch.Index(\n",
    "    text_fields = [\"question\", \"text\", \"section\"],\n",
    "    keyword_fields = [\"course\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58eb6e98-f760-4077-affe-809222e51319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x7433ec758250>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "125af2a8-6b74-494f-94ae-a61bcaa5682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section':0.5}\n",
    "\n",
    "    results = index.search(query=query, filter_dict = {'course': 'data-engineering-zoomcamp'},\n",
    "             boost_dict = boost, \n",
    "             num_results = 5\n",
    "            \n",
    "            )\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4582f89b-85a1-45bd-b645-7f1a982abba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template=\"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion:{doc['question']}\\nanswer:{doc['text']}\\n\\n\"\n",
    "\n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "\n",
    "    return prompt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "360a8aa4-0b7b-4195-b9a4-d0417b77c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(model=\"gpt-4o\", \n",
    "                                          messages = [{\"role\":\"user\", \"content\":prompt}])\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e24be2d-3076-4b2f-af62-2779fa5847c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'how do i run kafka'\n",
    "def rag(query):\n",
    "    search_results =search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "\n",
    "    print (answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b878db9-3d94-406d-9d2b-bdae3bd79be9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To run Kafka, based on the context provided for Java Kafka, you need to execute the following command in the terminal from your project directory:\n",
      "\n",
      "```sh\n",
      "java -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java\n",
      "```\n",
      "\n",
      "Replace `<jar_name>` with the actual name of your JAR file.\n",
      "\n",
      "If you are working with a Python Kafka producer and facing issues with the Kafka module, you should set up a virtual environment and install the required packages:\n",
      "\n",
      "1. Create and activate your virtual environment:\n",
      "\n",
      "```sh\n",
      "python -m venv env\n",
      "source env/bin/activate\n",
      "pip install -r ../requirements.txt\n",
      "```\n",
      "\n",
      "For Windows, the command to activate the environment is slightly different:\n",
      "\n",
      "```sh\n",
      "env\\Scripts\\activate\n",
      "```\n",
      "\n",
      "3. Once your virtual environment is set up and activated, you can run your Python Kafka producer. If you need to deactivate the virtual environment, use:\n",
      "\n",
      "```sh\n",
      "deactivate\n",
      "```\n",
      "\n",
      "Make sure all your Docker images, if used, are up and running before any attempts to run the Kafka Python files.\n"
     ]
    }
   ],
   "source": [
    "rag(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "473bbcac-3964-4acb-9cde-5bf6d206c18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ideal timeline to finish this course has not been explicitly provided in the FAQ database. However, it is mentioned that participants are expected to spend about 5 - 15 hours per week on the course. Using this information, you can estimate your own timeline based on the total duration of the course and your weekly availability. \n",
      "\n",
      "For instance, if the course is designed to be completed in 10 weeks and you allocate around 10 hours per week, you would likely align with the course schedule. Adjust accordingly if you plan to spend more or fewer hours per week.\n"
     ]
    }
   ],
   "source": [
    "rag(\"what is the ideal timeline to finish this course\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b95394da-ad97-4e7e-b805-60c5047c712d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To run Kafka components like producer, consumer, or KStreams in the terminal, you can follow these steps based on the language you are using.\n",
      "\n",
      "### Java Kafka:\n",
      "In the project directory, run:\n",
      "```\n",
      "java -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java\n",
      "```\n",
      "\n",
      "### Python Kafka:\n",
      "1. **Create a virtual environment** (only once):\n",
      "    ```\n",
      "    python -m venv env\n",
      "    source env/bin/activate\n",
      "    pip install -r ../requirements.txt\n",
      "    ```\n",
      "   For subsequent activations (every time you need the virtual env):\n",
      "    ```\n",
      "    source env/bin/activate\n",
      "    ```\n",
      "   To deactivate:\n",
      "    ```\n",
      "    deactivate\n",
      "    ```\n",
      "   Note: On Windows, the activation command is slightly different:\n",
      "    ```\n",
      "    env\\Scripts\\activate\n",
      "    ```\n",
      "2. **Ensure Docker images are up and running** before running the Python files.\n",
      "\n",
      "By adhering to these steps, you should be able to successfully run your Kafka components.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87e033b1-e2fd-4bca-abd9-5f5810d41a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bd48754-2e35-4f4b-9923-2d455fe8bb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_client = Elasticsearch('http://localhost:9200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2fa3886-6b26-4466-b151-6dc917398514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Elasticsearch(['http://localhost:9200'])>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83442607-29af-425f-84db-33527dee66e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'course-questions'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"} \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"course-questions\"\n",
    "\n",
    "es_client.indices.create(index=index_name, body = index_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fce35fc-9c4c-4c62-a63e-0935a176e793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.10.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0fcddf0-eb5a-40d2-8ff2-addd48fc6cda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 948/948 [00:21<00:00, 43.10it/s]\n"
     ]
    }
   ],
   "source": [
    "for doc in tqdm(documents):\n",
    "    es_client.index(index = index_name, document = doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4d2a168-0c3f-42e9-83cf-cadeeb739153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how do i run kafka'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f5d3a1bb-a325-442f-9131-4f463139b5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search(query):\n",
    "\n",
    "    search_query = {\n",
    "        \"size\": 5,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\"question^3\", \"text\", \"section\"],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"term\": {\n",
    "                        \"course\": \"data-engineering-zoomcamp\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = es_client.search(index = index_name, body= search_query)\n",
    "    result_docs  = []\n",
    "\n",
    "    for hit in response['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "\n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62c4401e-50f3-4d98-934a-7277013121a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"Answer: To run the provided code, ensure that the 'dlt[duckdb]' package is installed. You can do this by executing the provided installation command: !pip install dlt[duckdb]. If you’re doing it locally, be sure to also have duckdb pip installed (even before the duckdb package is loaded).\",\n",
       "  'section': 'Workshop 1 - dlthub',\n",
       "  'question': 'How do I install the necessary dependencies to run the code?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'In the project directory, run:\\njava -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Java Kafka: How to run producer/consumer/kstreams/etc in terminal',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\\nHaving this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\\nYou will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\\nRemember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\\nThis is also a great resource: https://dangitgit.com/',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'How do I use Git / GitHub for this course?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': \"Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\\nTo create a virtual env and install packages (run only once)\\npython -m venv env\\nsource env/bin/activate\\npip install -r ../requirements.txt\\nTo activate it (you'll need to run it every time you need the virtual env):\\nsource env/bin/activate\\nTo deactivate it:\\ndeactivate\\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.\",\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Module “kafka” not found when trying to run producer.py',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'You can check the version of your local spark using spark-submit --version. In the build.sh file of the Python folder, make sure that SPARK_VERSION matches your local version. Similarly, make sure the pyspark you pip installed also matches this version.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'How do I check compatibility of local and container Spark versions?',\n",
       "  'course': 'data-engineering-zoomcamp'}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elastic_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "286754b8-33a3-4b4b-ac01-387c001ac3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_rag(query):\n",
    "    search_results =elastic_search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "\n",
    "    print (answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd495baa-a513-46bc-89e4-5b129c135a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To run Kafka, follow these instructions depending on whether you are working with Java or Python:\n",
      "\n",
      "**Java Kafka:**\n",
      "1. Navigate to your project directory.\n",
      "2. Use the following command to run the necessary Java file:\n",
      "   ```bash\n",
      "   java -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java\n",
      "   ```\n",
      "\n",
      "**Python Kafka:**\n",
      "1. Create a virtual environment if you haven't already:\n",
      "   ```bash\n",
      "   python -m venv env\n",
      "   source env/bin/activate  # For MacOS/Linux\n",
      "   # For Windows:\n",
      "   # env\\Scripts\\activate\n",
      "   ```\n",
      "2. Install the required packages from `requirements.txt`:\n",
      "   ```bash\n",
      "   pip install -r requirements.txt\n",
      "   ```\n",
      "3. Make sure that Docker images are up and running if your setup requires them.\n",
      "\n",
      "By following these steps, you should be able to get Kafka up and running for both Java and Python environments as required by the course material.\n"
     ]
    }
   ],
   "source": [
    "elastic_rag(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b3bf4-1385-4d8e-a380-a72660251964",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
